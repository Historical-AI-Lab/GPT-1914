{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classification model\n",
    "\n",
    "Trains and tests a random forest model on page data after dividing it by volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('newfeaturematrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pagenum', 'pagefrac', 'backnum', 'backfrac', 'nlines', 'nwords',\n",
       "       'nalpha', 'fracalpha', 'nnumeric', 'fracnumeric', 'npunct', 'fracpunct',\n",
       "       'nupper', 'fracupper', 'nother', 'fracother', 'meanlinelen',\n",
       "       'sdlinelen', 'meanwordlength', 'startupper', 'verbs', 'top2000words',\n",
       "       'paratextwords', 'byofwords', 'fracprice', 'label', 'nwordsminusmean',\n",
       "       'wordlengthminusmean', 'linelenminusmean', 'top2000minusmean',\n",
       "       'nwordsminusprev', 'top2000minusprev', 'centerdist', 'centerdist^2',\n",
       "       'pagefrac^2', 'backfrac^2', 'htid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique htids in pages\n",
    "unique_htids = pages['htid'].unique().tolist()\n",
    "\n",
    "# Calculate the number of htids for training and testing\n",
    "num_train_htids = int(len(unique_htids) * 0.8)\n",
    "num_test_htids = len(unique_htids) - num_train_htids\n",
    "\n",
    "# Randomly select htids for training and testing\n",
    "train_htids = np.random.choice(unique_htids, size=num_train_htids, replace=False)\n",
    "test_htids = list(set(unique_htids) - set(train_htids))\n",
    "\n",
    "# Divide the pages dataframe into pages_train and pages_test\n",
    "pages_train = pages[pages['htid'].isin(train_htids)]\n",
    "pages_test = pages[pages['htid'].isin(test_htids)]\n",
    "\n",
    "# Delete the original pages dataframe\n",
    "del pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = pages_train['label']\n",
    "pages_train = pages_train.drop('label', axis=1)\n",
    "htids_train = pages_train['htid']\n",
    "pages_train = pages_train.drop('htid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 50, 'min_samples_split': 2, 'n_estimators': 250}\n",
      "Best cross-validation score: 0.9705630018644446\n"
     ]
    }
   ],
   "source": [
    "# Create the RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 250],\n",
    "    'max_depth': [None, 50],\n",
    "    'min_samples_split': [2, 3]\n",
    "}\n",
    "\n",
    "# Create the GroupKFold object\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=group_kfold, n_jobs=-1)\n",
    "\n",
    "# Perform grid search with grouped cross-validation\n",
    "grid_search.fit(pages_train, labels_train, groups=htids_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = pages_test['label']\n",
    "pages_test = pages_test.drop('label', axis=1)\n",
    "htids_test = pages_test['htid']\n",
    "pages_test = pages_test.drop('htid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9834111141613396\n",
      "0.9834111141613396\n"
     ]
    }
   ],
   "source": [
    "# Now we train a model using the best parameters found by grid search\n",
    "# on the train dataset, and evaluate it on the test dataset.\n",
    "\n",
    "clf = RandomForestClassifier(**grid_search.best_params_)    \n",
    "clf.fit(pages_train, labels_train)\n",
    "test_score = clf.score(pages_test, labels_test)\n",
    "print(\"Test score:\", test_score)\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(clf, 'models/RF_model4.pkl')\n",
    "\n",
    "# Save the test score\n",
    "with open('models/test_score4.txt', 'w') as f:\n",
    "    f.write(str(test_score))\n",
    "\n",
    "# Save the best parameters\n",
    "with open('models/best_params4.txt', 'w') as f:\n",
    "    f.write(str(grid_search.best_params_))\n",
    "\n",
    "# Test the model, clf\n",
    "predictions = clf.predict(pages_test)\n",
    "probabilities = clf.predict_proba(pages_test)\n",
    "print(clf.score(pages_test, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use grid_search.best_params_ to infer probabilities\n",
    "# for the training dataset, through cross-validation.\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "clf = RandomForestClassifier(**grid_search.best_params_)\n",
    "train_probabilities = cross_val_predict(clf, pages_train, labels_train, groups=htids_train, cv=group_kfold, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagenum</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>htid</th>\n",
       "      <th>label</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>uc2.ark+=13960=t1mg7h137</td>\n",
       "      <td>para</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uc2.ark+=13960=t1mg7h137</td>\n",
       "      <td>para</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>uc2.ark+=13960=t1mg7h137</td>\n",
       "      <td>para</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>uc2.ark+=13960=t1mg7h137</td>\n",
       "      <td>para</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>uc2.ark+=13960=t1mg7h137</td>\n",
       "      <td>para</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pagenum  wordcount                      htid label  probabilities\n",
       "0        0          0  uc2.ark+=13960=t1mg7h137  para          0.000\n",
       "1        1          0  uc2.ark+=13960=t1mg7h137  para          0.000\n",
       "2        2          0  uc2.ark+=13960=t1mg7h137  para          0.000\n",
       "3        3          0  uc2.ark+=13960=t1mg7h137  para          0.004\n",
       "4        4          0  uc2.ark+=13960=t1mg7h137  para          0.036"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with columns for pagenum, htids_train, labels_train, and train_probabilities\n",
    "df_train_results = pd.DataFrame({\n",
    "    'pagenum': pages_train['pagenum'],\n",
    "    'wordcount': pages_train['nwords'],\n",
    "    'htid': htids_train,\n",
    "    'label': labels_train,\n",
    "    'probabilities': train_probabilities[:, 1]  # Assuming the second column contains the probabilities for 'text'\n",
    "})\n",
    "\n",
    "df_train_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_results.to_csv('models/train_results3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(pages_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 200}\n",
      "Best cross-validation score: 0.9692771561182033\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "param_grid = {'C': [0.1, 1, 10, 50, 100, 150, 200, 300, 400, 500]}\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=group_kfold, n_jobs=-1)\n",
    "grid_search.fit(X, labels_train, groups=htids_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9469058511919095\n"
     ]
    }
   ],
   "source": [
    "# Now we train a model using the best parameters found by grid search\n",
    "# on the train dataset, and evaluate it on the test dataset.\n",
    "\n",
    "logreg = LogisticRegression(**grid_search.best_params_)    \n",
    "logreg.fit(X, labels_train)\n",
    "X_test = scaler.transform(pages_test)\n",
    "test_score = logreg.score(X_test, labels_test)\n",
    "print(\"Test score:\", test_score)\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(clf, 'models/LR_model2.pkl')\n",
    "\n",
    "# Save the test score\n",
    "with open('models/test_score2.txt', 'w') as f:\n",
    "    f.write(str(test_score))\n",
    "\n",
    "# Save the best parameters\n",
    "with open('models/best_params2.txt', 'w') as f:\n",
    "    f.write(str(grid_search.best_params_))\n",
    "\n",
    "# Test the model, clf\n",
    "LRpredictions = logreg.predict(X_test)\n",
    "LRprobabilities = logreg.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(**grid_search.best_params_)\n",
    "train_probabilities = cross_val_predict(logreg, X, labels_train, groups=htids_train, cv=group_kfold, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagenum</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>htid</th>\n",
       "      <th>label</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>nyp.33433071387207</td>\n",
       "      <td>para</td>\n",
       "      <td>0.107463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nyp.33433071387207</td>\n",
       "      <td>para</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nyp.33433071387207</td>\n",
       "      <td>para</td>\n",
       "      <td>0.006158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>nyp.33433071387207</td>\n",
       "      <td>para</td>\n",
       "      <td>0.007745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>nyp.33433071387207</td>\n",
       "      <td>para</td>\n",
       "      <td>0.056439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pagenum  wordcount                htid label  probabilities\n",
       "178        0          5  nyp.33433071387207  para       0.107463\n",
       "179        1          0  nyp.33433071387207  para       0.003639\n",
       "180        2          0  nyp.33433071387207  para       0.006158\n",
       "181        3          0  nyp.33433071387207  para       0.007745\n",
       "182        4          0  nyp.33433071387207  para       0.056439"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with columns for pagenum, htids_train, labels_train, and train_probabilities\n",
    "df_train_results = pd.DataFrame({\n",
    "    'pagenum': pages_train['pagenum'],\n",
    "    'wordcount': pages_train['nwords'],\n",
    "    'htid': htids_train,\n",
    "    'label': labels_train,\n",
    "    'probabilities': train_probabilities[:, 1]  # Assuming the second column contains the probabilities for 'text'\n",
    "})\n",
    "\n",
    "df_train_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_results.to_csv('models/train_results2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
