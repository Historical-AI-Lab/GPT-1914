{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook takes as input a folder containing .txt Hathi book files. It removes newline characters so fasttext can process, strips first and last 8% of text (as a rough front/back matter trimmer), fuses sentences with characters of n length to their neighbors, and applies a fasttext English detection on the sentences. It also calculates each sentence's distance from the center of the book.\n",
        "\n",
        "The final output is a tsv file that has the following columns:\n",
        "\n",
        "\n",
        "*   Filename\n",
        "*   previous two sentences\n",
        "*   sentence (the sentence we are interested in)\n",
        "*   next two sentences\n",
        "*   whether or not fasttext determined the sentence was English (T/F)\n",
        "*   the English probability for the sentence\n",
        "*   The non-English language with the highest fasttext probability\n",
        "*   The probability for that non-English language (will outperform English if the sentence is False for English)\n",
        "*   Center-distance\n"
      ],
      "metadata": {
        "id": "PXHW5Pqep8CC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhI6ahiOp1Uf",
        "outputId": "7c8ae61a-b667-4d24-d0b2-77e67a5e257d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAkgwTi6rVSd",
        "outputId": "a2057f3e-16af-4667-8c5a-7dbb523a2c9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to remove front and back 8% of text\n",
        "\n",
        "def remove_header_footer(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
        "\n",
        "    for file in input_files:\n",
        "        file_path = os.path.join(input_folder, file)\n",
        "\n",
        "        with open(file_path, 'r', encoding=\"utf-8\") as j:\n",
        "            book = j.read()\n",
        "\n",
        "        split_book = book.split('<pb>')\n",
        "\n",
        "        book_len = len(split_book)\n",
        "        #pages_to_remove = int(book_len * 0.08)\n",
        "        start= int(book_len * 0.08)\n",
        "        end = book_len - start\n",
        "\n",
        "        if book_len > 0:\n",
        "            #split_book = split_book[pages_to_remove:-pages_to_remove]\n",
        "            split_book = split_book[start:end]\n",
        "\n",
        "        new_book = '<pb>'.join(split_book)\n",
        "\n",
        "\n",
        "        output_files = os.path.join(output_folder, file)\n",
        "        with open(output_files, 'w', encoding=\"utf-8\") as j:\n",
        "            j.write(new_book)\n",
        "\n",
        "input_folder_path = \"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/headerless_all\"\n",
        "output_folder_path = \"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/headerless_all_nofrontback\"\n",
        "remove_header_footer(input_folder_path, output_folder_path)"
      ],
      "metadata": {
        "id": "3bciZhRqrEMV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An extra preprocessing function that removes newline characters. This is necessary to run fasttext because otherwise get an error about the \\n\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    text = re.sub(r'\\n', \" \", text)\n",
        "    #text = re.sub(r'\\s+', ' ', text)\n",
        "    #text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "Zk917bMrrEI6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the preprocessing and saving the result\n",
        "\n",
        "import re\n",
        "\n",
        "def process_files_in_folder(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            processed_text = preprocess_text(text)\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(processed_text)\n",
        "\n",
        "# Specify the path to your folder\n",
        "folder_path = \"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/headerless_all_nofrontback\"\n",
        "\n",
        "process_files_in_folder(folder_path)"
      ],
      "metadata": {
        "id": "9ej3ID6OrEGZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes sentences as input and if the sentence is fewer than 5 characters,\n",
        "# it fuses it to the next or preceding sentence.\n",
        "\n",
        "def fuse_one_word_sentences(sentences, threshold=5):\n",
        "    fused_sentences = []\n",
        "    i = 0\n",
        "    while i < len(sentences):\n",
        "        if len(sentences[i]) <= threshold:\n",
        "            fused_sentence = sentences[i]\n",
        "            while i + 1 < len(sentences) and len(sentences[i + 1]) <= threshold:\n",
        "                i += 1\n",
        "                fused_sentence += \" \" + sentences[i]\n",
        "            if i + 1 < len(sentences):\n",
        "                fused_sentence += \" \" + sentences[i + 1]\n",
        "                i += 1\n",
        "            else:\n",
        "                if fused_sentences:\n",
        "                    fused_sentences[-1] += \" \" + fused_sentence\n",
        "                else:\n",
        "                    fused_sentences.append(fused_sentence)\n",
        "                i += 1\n",
        "                continue\n",
        "            fused_sentences.append(fused_sentence)\n",
        "        else:\n",
        "            fused_sentences.append(sentences[i])\n",
        "        i += 1\n",
        "    return fused_sentences\n",
        "\n",
        "#try:\n",
        "sentences = [\"hi.\", \"hi.             \", \"here's another.\", \"And...\", \"What about a third sentence.\", \".\", \",\", \".\", \"9          4\", \"9.........4\", \"Now\", \"Now?\", \"Now!!\", \"But not now.\"]\n",
        "fuse_one_word_sentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1UErmuyrToF",
        "outputId": "718ee456-7936-4c37-a7a4-650efeafb605"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi. hi.             ',\n",
              " \"here's another.\",\n",
              " 'And...',\n",
              " 'What about a third sentence.',\n",
              " '. , . 9          4',\n",
              " '9.........4',\n",
              " 'Now Now? Now!! But not now.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The new fasttext function. This function looks for the whether the first label (labels[0]) is english or not,\n",
        "# for a T/F boolean. It also looks for the english label and assigns the probability for that label to\n",
        "# \"english_prob\". Finally, it looks for the top probability of a non-english\n",
        "# label, and assigns that label and probability to the non-English language/probability feature.\n",
        "\n",
        "!pip install fasttext\n",
        "import fasttext\n",
        "\n",
        "# Downloading and loading fasttext model\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "model = fasttext.load_model('lid.176.bin')\n",
        "\n",
        "def find_english_probability(sentence):\n",
        "    predictions = model.predict(sentence, k=-1)  # Use k=-1 to get all predictions\n",
        "    labels = predictions[0]\n",
        "    probabilities = predictions[1]\n",
        "\n",
        "    english_probability = 0.0\n",
        "    top_non_english_lang = ''\n",
        "    top_non_english_probability = 0.0\n",
        "\n",
        "    for label, probability in zip(labels, probabilities):\n",
        "        if label == '__label__en':\n",
        "            english_probability = probability\n",
        "        else:\n",
        "            if probability > top_non_english_probability:\n",
        "                top_non_english_lang = label\n",
        "                top_non_english_probability = probability\n",
        "\n",
        "    is_english = labels[0] == '__label__en'\n",
        "\n",
        "    return is_english, english_probability, top_non_english_lang, top_non_english_probability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rEgLoJPrTjM",
        "outputId": "993770bd-9a7e-4249-8613-ecf5ddbfc75f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246766 sha256=46e8c151b88f97c29853186d9e3ec39fd4cf473e896bca2d2613c190d9f2ab9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.1\n",
            "--2024-08-05 18:40:39--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.59, 13.227.219.70, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   155MB/s    in 0.8s    \n",
            "\n",
            "2024-08-05 18:40:40 (155 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here's the master function that takes a folder with txt files that have had newline characters\n",
        "# removed and 8% removed. The function tokenizes by sentence, fuses short sentences, and then\n",
        "# applies fasttext. It also calculates\n",
        "# the sentence's center distance (distance from the center of the book). And it gives the preceding TWO\n",
        "# sentences and next TWO sentences, relative to the sentence in question. It outputs the\n",
        "# first 9 columns of the tsv.\n",
        "\n",
        "################################################# TEST #########################################\n",
        "\n",
        "def find_noneng_sent(input_folder):\n",
        "    all_sentences = []\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith('.txt'):\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r', encoding=\"utf-8\") as j:\n",
        "                book = j.read()\n",
        "                sentences = sent_tokenize(book)\n",
        "                fused_sentences = fuse_one_word_sentences(sentences)\n",
        "                total_sentences = len(fused_sentences)\n",
        "\n",
        "                for i, sent in enumerate(fused_sentences):\n",
        "                    is_english, english_probability, non_english_lang, non_english_probability = find_english_probability(sent)\n",
        "\n",
        "                    prev_sent_2 = fused_sentences[i-2] if i > 1 else \"\"\n",
        "                    prev_sent_1 = fused_sentences[i-1] if i > 0 else \"\"\n",
        "                    prev_sents = (prev_sent_2 + \" \" + prev_sent_1).strip()\n",
        "\n",
        "                    next_sent_1 = fused_sentences[i+1] if i < total_sentences-1 else \"\"\n",
        "                    next_sent_2 = fused_sentences[i+2] if i < total_sentences-2 else \"\"\n",
        "                    next_sents = (next_sent_1 + \" \" + next_sent_2).strip()\n",
        "\n",
        "                    # calculate center distance here\n",
        "                    center_distance = abs((i / total_sentences) - 0.5)\n",
        "\n",
        "                    all_sentences.append((file, prev_sents, sent, next_sents, is_english, english_probability, non_english_lang, non_english_probability, center_distance))\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "# Running this on the folder of hathi txt files\n",
        "input_folder = \"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/headerless_all_nofrontback\"\n",
        "results = find_noneng_sent(input_folder)\n"
      ],
      "metadata": {
        "id": "ndR9ObA3l3Oo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing the results to a tsv\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(\"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/all_hathi_sents_macro1.tsv\", 'w', newline='', encoding=\"utf-8\") as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    writer.writerow(['file', 'prev_sent', 'sent', 'next_sent', 'is_english', 'english_prob', 'non_english_lang', 'non_english_prob', 'center_dist'])\n",
        "    writer.writerows(results)\n"
      ],
      "metadata": {
        "id": "7XRiPkMYrTdU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Et4aXzPNrTa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6eQwHGHrTYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function below is old and shouldn't be used, but I'm keeping it for reference's sake!"
      ],
      "metadata": {
        "id": "anNzFoh0pgfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here's the master function that takes a folder with txt files that have had newline characters removed and 8% removed. The function\n",
        "# tokenizes by sentence, fuses short sentences, and then applies fasttext. It outputs the first 8 columns of the tsv.\n",
        "\n",
        "def find_noneng_sent(input_folder):\n",
        "    all_sentences = []\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith('.txt'):\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r', encoding=\"utf-8\") as j:\n",
        "                book = j.read()\n",
        "                sentences = sent_tokenize(book)\n",
        "                fused_sentences = fuse_one_word_sentences(sentences)\n",
        "\n",
        "                for i, sent in enumerate(fused_sentences):\n",
        "                    is_english, english_probability, non_english_lang, non_english_probability = find_english_probability(sent)\n",
        "                    prev_sent = fused_sentences[i-1] if i > 0 else \"\"\n",
        "                    next_sent = fused_sentences[i+1] if i < len(fused_sentences)-1 else \"\"\n",
        "                    all_sentences.append((file, prev_sent, sent, next_sent, is_english, english_probability, non_english_lang, non_english_probability))\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "# Example usage\n",
        "input_folder = \"/content/drive/MyDrive/UIUC_Summer2024/RA_Underwood/GPT1914/headerless_test_nofrontback\"\n",
        "results = find_noneng_sent(input_folder)"
      ],
      "metadata": {
        "id": "YQoLWCUIrTfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}